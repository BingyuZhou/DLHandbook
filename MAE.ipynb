{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "%matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from einops.layers.torch import Rearrange\n",
    "from einops import rearrange\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx][0] #only use img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/home/bingyu/DLHandbook/dataset/coco/train2017\"\n",
    "annFile = \"/home/bingyu/DLHandbook/dataset/coco/annotations/instances_train2017.json\"\n",
    "transform = torchvision.transforms.Compose(\n",
    "    [transforms.Resize((224, 224)), transforms.ToTensor()]\n",
    ")\n",
    "coco = torchvision.datasets.CocoDetection(root, annFile, transform=transform)\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    coco, [80000, 118287 - 80000]\n",
    ")\n",
    "train_dataset = CocoDataset(train_dataset)\n",
    "val_dataset = CocoDataset(val_dataset)\n",
    "\n",
    "train_sampler = torch.utils.data.SubsetRandomSampler(range(2000))\n",
    "val_sampler = torch.utils.data.SubsetRandomSampler(range(5000,5300))\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=8, num_workers=4, sampler=train_sampler,\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=8, num_workers=4, sampler=val_sampler,\n",
    ")\n",
    "\n",
    "print(f\"train_loader:{len(train_loader)}, val_loader:{len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in train_loader:\n",
    "    fig, ax = plt.subplots(1,4, figsize=(15,10))\n",
    "    ax[0].imshow(img[0].permute(1,2,0))\n",
    "    ax[1].imshow(img[2].permute(1,2,0))\n",
    "    ax[2].imshow(img[4].permute(1,2,0))\n",
    "    ax[3].imshow(img[7].permute(1,2,0))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dim_head=64, dropout=0):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.dim_head = dim_head\n",
    "\n",
    "        inner_dim = heads * dim_head\n",
    "\n",
    "        self.key = nn.Linear(dim, inner_dim)\n",
    "        self.query = nn.Linear(dim, inner_dim)\n",
    "        self.value = nn.Linear(dim, inner_dim)\n",
    "\n",
    "        self.proj = nn.Linear(inner_dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "\n",
    "        key = (\n",
    "            self.key(x).view(B, N, self.heads, self.dim_head).transpose(1, 2)\n",
    "        )  # B, nh, N, D\n",
    "        query = self.query(x).view(B, N, self.heads, self.dim_head).transpose(1, 2)\n",
    "        value = self.value(x).view(B, N, self.heads, self.dim_head).transpose(1, 2)\n",
    "\n",
    "        scale = self.dim_head ** -0.5\n",
    "        att = (query @ key.transpose(-2, -1)) * scale  # B, nh, N, N\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = att @ value  # B, nh, N, D\n",
    "        att = att.transpose(1, 2).reshape(B, N, self.heads * self.dim_head)\n",
    "\n",
    "        out = self.proj(att)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, depth, num_heads, dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.layers = []\n",
    "\n",
    "        for _ in range(depth):\n",
    "            atten = nn.Sequential(nn.LayerNorm(dim), Attention(dim, num_heads))\n",
    "            mlp = nn.Sequential(\n",
    "                nn.LayerNorm(dim),\n",
    "                nn.Linear(dim, hid_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(hid_dim, dim),\n",
    "                nn.Dropout(dropout),\n",
    "            )\n",
    "            self.layers.append(nn.ModuleList([atten, mlp]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for atten, mlp in self.layers:\n",
    "            x = atten(x) + x\n",
    "            x = mlp(x) + x\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAE(nn.Module):\n",
    "    def __init__(self, img_h, img_w, patch_h, patch_w, emb_dim, mask_ratio) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_h = patch_h\n",
    "        self.patch_w = patch_w\n",
    "        self.mask_ratio = mask_ratio\n",
    "\n",
    "        num_patch = (img_h // patch_h) * (img_w // patch_w)\n",
    "        patch_dim = 3 * patch_h * patch_w\n",
    "\n",
    "        # self.path_embed = nn.Sequential(\n",
    "        #     nn.Conv2d(\n",
    "        #         in_channels=3,\n",
    "        #         out_channels=emb_dim,\n",
    "        #         kernel_size=(patch_h, path_w),\n",
    "        #         stride=(patch_h, path_w),\n",
    "        #     ),\n",
    "        #     nn.Flatten(),\n",
    "        # )\n",
    "\n",
    "        self.to_patch = Rearrange(\n",
    "            \"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\", p1=patch_h, p2=patch_w\n",
    "        )\n",
    "\n",
    "        self.patch_embed = nn.Linear(patch_dim, emb_dim)\n",
    "\n",
    "        self.position_encoding = nn.Parameter(torch.randn(1, num_patch, emb_dim))\n",
    "\n",
    "        self.encoder = Transformer(depth=3, num_heads=2, dim=emb_dim, hid_dim=768, dropout=0.0)\n",
    "        self.encoder_norm = nn.LayerNorm(emb_dim)\n",
    "        self.decoder = Transformer(depth=1, num_heads=2, dim=emb_dim, hid_dim=2*emb_dim, dropout=0.0)\n",
    "        self.decoder_norm = nn.LayerNorm(emb_dim)\n",
    "        self.mask_token = nn.Parameter(torch.randn(emb_dim))\n",
    "\n",
    "        self.to_pixels = nn.Linear(emb_dim, patch_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B,C,H,W\n",
    "        \"\"\"\n",
    "        # patch embedding\n",
    "        x = self.to_patch(x)  # B,N,C\n",
    "        patch_emb = self.patch_embed(x)  # B,N,D\n",
    "\n",
    "        # position encode\n",
    "        patch_emb += self.position_encoding\n",
    "\n",
    "        # masking\n",
    "        B, N, *_ = patch_emb.shape\n",
    "        num_masked = int(N * self.mask_ratio)\n",
    "        rand_indice = torch.rand(B, N).argsort(dim=-1)\n",
    "        masked_indice = rand_indice[:, :num_masked]\n",
    "        unmasked_indice = rand_indice[:, num_masked:]\n",
    "\n",
    "        # encoder\n",
    "        encoded_token = self.encoder(patch_emb[torch.arange(B)[:,None], unmasked_indice, :])\n",
    "        encoded_token = self.encoder_norm(encoded_token)\n",
    "\n",
    "        # decoder\n",
    "        mask_tokens = self.mask_token.repeat(B, num_masked, 1)\n",
    "        mask_tokens += self.position_encoding[:, masked_indice, :].squeeze()\n",
    "\n",
    "        encoded_token += self.position_encoding[:, unmasked_indice, :].squeeze()\n",
    "\n",
    "        decoder_tokens = torch.cat((encoded_token, mask_tokens), dim=1)  # B,N,D\n",
    "        decoder_tokens = self.decoder(decoder_tokens)\n",
    "        decoder_tokens = self.decoder_norm(decoder_tokens)\n",
    "\n",
    "        masked_tokens = decoder_tokens[:, -num_masked:]\n",
    "        masked_pixel = self.to_pixels(masked_tokens)\n",
    "\n",
    "        masked_patches = x[torch.arange(B)[:,None], masked_indice]\n",
    "        \n",
    "        loss = F.mse_loss(masked_pixel, masked_patches)\n",
    "\n",
    "        return loss, masked_pixel.detach(), masked_patches.detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(gt_img, pred_patches, target_patches):\n",
    "    print(f\"pred_patches: {pred_patches.shape}, target_patches: {target_patches.shape}\")\n",
    "    fig1, ax1 = plt.subplots(1,1)\n",
    "    ax1.imshow(gt_img.permute(1,2,0))\n",
    "    fig2, ax2 = plt.subplots(6,6)\n",
    "    for i in range(36):\n",
    "        ax2[i//6, i%6].imshow(pred_patches[i])\n",
    "    fig3, ax3 = plt.subplots(6,6)\n",
    "    for i in range(36):\n",
    "        ax3[i//6, i%6].imshow(target_patches[i])\n",
    "    fig1.show()\n",
    "    fig2.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MAE(224, 224, 16, 16, 128, mask_ratio=0.75)\n",
    "\n",
    "print(model)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-4, betas=(0.9, 0.95))\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "\n",
    "for epoch in range(100):\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    for imgs in tqdm(train_loader):\n",
    "        loss, *_ = model(imgs)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss/len(train_loader)\n",
    "\n",
    "        # masked_patch_pred = rearrange(masked_pixel, 'b n (p1 p2 c) -> b n p1 p2 c', p1=16, p2=16)\n",
    "        # masked_patches_target = rearrange(masked_patches, 'b n (p1 p2 c) -> b n p1 p2 c', p1=16, p2=16)\n",
    "\n",
    "        # plot(imgs[0], masked_patch_pred[0], masked_patches_target[0])\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        epoch_val_loss = 0\n",
    "        model.eval()\n",
    "        for imgs in val_loader:\n",
    "            val_loss, masked_pixel, masked_patches = model(imgs)\n",
    "            epoch_val_loss += val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch: {epoch}, lr: {scheduler.get_lr()}train_loss: {epoch_loss}, val_loss: {epoch_val_loss}\")\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for imgs in train_loader:\n",
    "        _, masked_pixel, masked_patches = model(imgs)\n",
    "        masked_patch_pred = rearrange(masked_pixel, 'b n (p1 p2 c) -> b n p1 p2 c', p1=16, p2=16)\n",
    "        masked_patches_target = rearrange(masked_patches, 'b n (p1 p2 c) -> b n p1 p2 c', p1=16, p2=16)\n",
    "\n",
    "        plot(imgs[0], masked_patch_pred[0], masked_patches_target[0])\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ed013081175e39fd34521177943bca7b4f1c0dde1e3dd7e487f60cb41b81e3a8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('pytorch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
